# ML -> DL
## 신경망 학습의 핵심적인 알고리즘
1. 선형 회귀(예측)
2. 로지스틱 회귀(분류)

# 럭키백
- Hint 확률 제공

# 확률 계산하기
## K-최근집 이웃 다중 분류
- KNN을 이용해 분류 확신성(확률)을 출력.

## 이진 분류 - 로지스틱 회귀
- 확률을 0~1사이로 매핑(0.5를 기준으로 양성과 음성으로 분류할 수 있음.)
- 점들의 특성을 정확하게 담아내려면 직선이 아니라 S자 형태가 됨.
- 로지스틱 회귀는 선형 회귀와 마찬가지로 적절한 선을 그려가는 과정.
- 시그모이드, 로지스틱 함수 방정식 => 1/1+e^(-x) = > 1/1+e^(-(ax+b))
- a값이 커지면 경사도가 완만해짐.
- b값이 양이면 오른쪽으로, 음이면 왼쪽으로 그래프가 이동함.

1. boolean indexing
- 뽑아낼 인덱스에 true처리 뽑아내지 않을 인덱스에 false 처리를 한다.

```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```



## 다중분류 - 로지스틱 회귀
1. LogisticRegression은 여러 알고리즘을 사용함,
2. 소프트맥스는 그림에서와 같이 총합이 1인 형태로 바꿔서 계산해 주는 함수
3. 합계가 1인 형태로 변환시 큰 값이 두드러지게 나타나고 작은 값은 더 작아짐.


# 정리
- 분류 모델은 예측 뿐만 아니라 예측의 근거가 되는 확률 출력 가능.
- 확률은 분류 모델이 얼마나 예측을 화긴하는지를 나타냄.
- KNN 모델이 확률을 출력할 수 있지만, 이웃 샘플의 클래스 비율이므로 항상 정해진 확률만 출력 - 한계점
- KNN 한계를 극복할 수 있는 분류 모델 중 하나로 로지스틱 회귀 모델
- 선형회귀처럼 계산한 값을 그대로 출력하는 것이 아니라 이 값을 0~1 사이로 압축, 즉 0~ 100% 사이의 확률이라고 볼 수 있음.
- 로지스틱회귀는 이진 분류에서는 하나의 선형방정식을 훈련함
- 다중 분류일 경우, 클래스 개수만큼 방정식을 훈련함.
- 각 방정식의 출력값을 소프트맥스 함수를 이용해. 전체 클래스에 대한 합이 1이 되도록 만듬
- 이 값을 각 클래스에 대한 확률이 됨.

# 경사하강법
1. 점진적 학습
- 모델은 그대로고 데이터를 점진적으로 학습시킴.
2. OonlineLearning 온라인학습 - 동적학습
3. 비유
- 학습의 방법이고, 손실과 비용을 줄이는 최적화 알고리즘.
- ax+b에서 손실을 줄이는 최적의 a와 b를 찾아간다.

# 손실(loss) 줄이기 - 반복 방식(try it first)
- 머신러닝 모델이 반복을 통해 어떻게 손실을 줄이는지 이해

# 확률적 경사하강법(SGD)
- SGD는 머신러닝 모델이 아니고, 모델을 훈련시키는 최적화 방법
- 확률적이란 단어는 무작위하게, 랜덤하게 라는 의미

# 경사하강법과 모델 학습 - 기울기 변화
- 기울기의 변화로 적합한 값을 찾는다.
- 기울기의 변화를 포물선으로 나타낼 수 있다.

# 경사 하강법(gradient decent) 구현을 위한 미분 방정식
- 지금까지의 내용 정리
- 평균제곱오차(MSE)식


# MSE / RMSE
1. MSE
- 선형회귀에서 사용
2. RMSE 
- root MSE => sqrt(MSE) 

# 로지스틱 손실 함수
1. 분류
- 분류상황에서 정확도로 모델 성능을 보고, 최적화는 로지스틱 손실함수로 나누어져 있음.
이유는 정확도는 미분할 수 없기 때문ㅇ ㅔ경사하강법을 사용할 수 없음.
- 경사하강법을 사용할 수 있는 다른 대안 마련 -> 로지스틱 손실함수
- 실제 값이 1일때 -log h 그래프를 쓰고,
- 0일 때 -log(1-h) 그래프를 써야 함.
2. 점화식

- C(H(x),y) { y == 1 이면 -log(H(x)), y == 0 이면 -log(1-H(x))}
- => -{ylogh +(1-y)log(1-h)}

3. SGDClassfier
- 확률적 경사하강법 분류


# Epoch vs Overfitting / Underfitting
1. Overfitting / Underfitting
- Overfitting 문제 해결을 위해, 규제(일반화)를 적용
- 규제 강도가 작으면, 훌년셋에 맞춤.

2. Epoch
- 많이하면 훈련셋에 맞춤, 테스트셋에서는 낮아지는 Overfitting이 발생.
- 적게하면 학습이 충분히 되지 않은 Underfitting이 발생.
- 테스트셋 성능이 떨어지기 전 epoch 값을 찾아내서, 거기까지만 훈련을 하여야함.
```python
x = train_set
y = test_set
rate = 0 # early stopping
next_rate = 0 
if rate > next_rate:
    next_rate = x_rate-y_rate
    rate = next_rate
```
- 이를 Early Stopping 조기 종료라고 함.
