# 인공신경망

1. 데이터셋 구하기.
```python
from tensorflow import keras

# mnist 데이터셋.
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() # 훈련세트, 테스트세트 분리
```
- keras Dataset 내장 데이터셋(https://keras.io/api/datasets)

2. 데이터의 분류
- train data 와 test data 분리
- train data를 train data, validation data 로 분리
- 교차검증 훈련 => test 데이터로 테스트.


3. 훈련 데이터셋 이미지 출력
```python
# 10 개 훈련 데이터셋 이미지 출력
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')
    axs[i].axis('off')
plt.show()

# 10개 훈련 데이터셋 타겟값 출력
print([train_target[i] for i in range(10)])

# 타겟값별 데이터 수 출력
import numpy as np

print(np.unique(train_target, return_counts=True))
```

4. 로지스틱 회귀로 패션 아이템 분류하기
```python
# 훈련 데이터 변환(각 이미지를 1차원으로)
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)
print(train_scaled.shape)

# LogisticRegression() 대신 확률적경사하강법(SGD)이용한 로지스틱회귀 분류 모델
# loss='log'는 이진분류는 sigmoid, 다중분류는 OvR(OvA) 이진분류 반복 후 softmax(10개의 z값)
from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier # optimizer

sc = SGDClassifier(loss='log', max_iter=5, random_state=42)

# 생성된 모델(sc)에 교차검증(기본 5-fold, 검증 데이터 82% 정확도)
scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))
```

5. 로지스틱회귀와 인공신경망(ANN)비교
- 인공신경망은 확률적경사하강법을 사용하는 로지스틱회귀와 같음.
- 확률적 경사하강법(Sigmoid, Softmax)
- 입력층(inupt layer)
- 출력층(output layer)

6. 인공신경망
```python
# 텐서플로와 케라스
import tensorflow as tf
from tensorflow import keras

# 인공신경망으로 모델 만들기
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled.shape, train_target.shape)
print(val_scaled.shape, val_target.shape)
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
model = keras.Sequential(dense)

# 인공신경망으로 패션 아이템 분류하기.
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

# 분류 예시
print(train_target[:10])

# 훈련 epoch=5
model.fit(train_scaled, train_target, epochs=5)

# 평가
model.evaluate(val_scaled, val_target)
```
## 엔트로피
- 엔트로피 : 불확실성의 척도
- 엔트로피가 높다는 것은 정보가 많고, 확률이 낮다는 것을 의ㅣㅁ

## 크로스 엔트로피
- 실제 분포 q에 대하여 알지 못하는 상태에서, 모델링을 통하여 구한 분포인 p를 통하여 q를 예측하는 것
- p(x)와 q(x)가 서로 교차해서 곱한다는 의미에서 크로스 엔트로피라고 함
- 머신러닝의 모델은 몇%의 확률로 예측.
- fit() - 모델 훈련(epoch 5, loss는 줄어들면서 정확도는 높아짐.)
- keras는 기본적으로 Mini-batch 경사하강법 사용(기본 batch_size는 32)



## 인공신경망의 작동방식(퍼셉트론)
1. 입력값(input layer)
2. 가중치(w)
3. 활성함수(f(x))
4. 출력값(output layer)
- 들어온 입력값과 각 가중치를 곱하고
- 곱해진 값들을 모두 더한 후
- 그 값이 한계값을 넘어서면 1 아니면 0을 출력.
- 원하는 입력값 결정.
- 가중치가 업데이트 되어가는 과정이 인공지능의 학습.

5. 해결 불가능한 과제 (XOR연산)
- 두 입력값이 같으면0, 다르면 1을 출력
- 해결방안 : 다중 퍼셉트론
- 오차가 생기면 거슬러 올라가 결정하는 오차역전파법.
- XOR 문제를 해결하려면 새로운 접근이 필요함.

6. 딥러닝
- 스스로 특징을 추출하고 분류한다.
- y = wx +b(w는 가중치, b는 바이어스)


## 다층 퍼셉트론(MLP)
- 은닉층을 포함해 가중치와 바이어스를 2차원 배열 나타낼 수 있음.
- XOR문제 해결
- XOR를 해결하는 가중치와 바이어스의 조합은 무수히 많음.

## 오차 역전파
- 원하는 XOR 문제의 정답이 도출됨.
- 신경망 내부의 가중치는 오차 역전파 방법을 사용해 수정.
- XOR에서 정답에 해당하는 w,b를 미리 알아본 후 이를 집어넣음
- 실제 프로젝트에서는 최적의 가중치와 바이어스를 찾아나가는 것.
이때 경사하강법 사용

## 드롭아웃
- 신경망에서 훈련시 Overfitting을 줄이기 위해 특화된 방법.
- 일정부분의 신경망만 적용시켜서 Overfitting을 줄인다.

## 모델 저장과 복원
1. 모델 저장
- 모델 파라미터(가중치, 절편) 저장 save_weights()
- 파일 확장자를 .h5이면 HDF5 포맷으로 저장
model.save_weights('model-weights.h5)

- 모델 구조와 파라미터 함께 저장 save()
- 파일 확장자를 .h5이면 HDF5 포맷으로 저장
model.save('model-whole.h5)

2. 모델 복원.
model.load('model-weights.h5')

## 조기 종료(early stopping)
- 최상의 점수를 얻으면 epoch 설정에 상관없이 종료
- 콜백을 2개 선언 (ModelCheckpoint(), EarlyStopping())

- 콜백을 포함해서 훈련
```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5')
early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,
                                                  restore_best_weights=True)
history = model.fit(train_scaled, train_target, epochs=20, verbose=0, 
                    validation_data=(val_scaled, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```


## 합성곱
합성곱 신경망에서는 Dense와는 다르게, 모든 입력에 가중치를 대응하는 것이 아니라, 일부만 <br>
그림에서, 입력이 10개이지만, 가중치는 3개만 사용(가중치가 하이퍼 파라메터가 됨.)<br>
눈에서 시각 입력을 받는 뉴런이 일반적으로 단순한 직선 모서리에 제일 민감하다는 것을 발견<br>

1. 흐름
가장자리 -> 곡선 -> 질감 -> 물체 일부분 -> 물체의 종류 인식

- 저수준 특징
가장자리 -> 곡선

- 고수준 특징
질감 -> 물체 일부분

- 복잡한 추론
물체의 종류 인식

## 2차원 합성곱
2차원 데이터를 그대로 입력으로 사용 가능<br>
슬라이딩 방식(왼쪽에서 오른쪽으로, 위에서 아래로)<br>
특성맵<br>
입력 데이터를 필터가 순회하며 계산되는 합성곱으로 구성하는 행렬<br>

## 여러개의 필터
서로 다른 특징을 얻기 위해서 여러 개의 필터 사용<br>
keras에서 2차원 합성곱 층(첫번째 매개변수는 필터의 개수)<br>
커널 크기는 일반적으로 (3,3) 이나 (5,5)를 많이 사용<br>
- ex) block1_conv1 (Conv2D)(None, 224, 224, 64), 224*224 depth = 64

## 패딩
입력 배열 주위에 가상의 원소(주로 0으로 채움)로 채우는 것을 padding 이라고 함<br>
패딩이 되면, 커널 적용 차이가 줄어드는 효과가 있음<br>
또한 입력 배열을 둘러싸면 크기가 6*6이 되고 특성 맵은 4*4가 됨.<br>
이럴 경우 특성 맵이 원본 입력과 동일한 크기인 4*4가 되어 이를 same padding이라 함.<br>

## 패딩 목적
외곽 데이터에 중요한 특성이 있다면 반영되지 못 할 수 있음<br>
이러한 문제를 극복하기 위해, 외곽에 실제값이 아닌 값으로 둘러쌈, 이를 패딩이라고 함.<br>

## 스트라이드(stride)
합성곱 연산은 좌우, 위아래로 한 칸씩 이동함.<br>
그림과 같이, 두 칸씩 이동도 가능함, 이 경우 특성 맵은 더 작아지겠음<br>
keras의 스트라이드 설정(기본값 1), 이동 크기를 튜플로 설정 (1,1)<br>
- 합성곱 연산을 시행할 때 몇 칸씩 이동할지를 결정하는 변수
```python
keras.layers.Conv2D(10, kernel_size=(3,3), activation='relu', padding='same',strides = 1) 
# kernel_size 3*3크기로 탐색, strides = 이동할 칸
```

## 풀링(pooling)
합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행<br>
최대 풀링과 평균 풀링<br>
전형적으로 2*2 풀링을 하며, stride도 2가 됨(풀링은 겹쳐서 수행하지 않음)<br>

```python
keras.layers.MaxPooling2D(2) # AveragePolling2D()
keras.layers.MaxPooling2D(2, strides=2, padding='valid')
```

# 합성곱 신경망
CNN의 중요 개념 : 합성곱 층, 필터, 패딩, 스트라읻, 폴링<br>
그림은 전형적인 CNN 구조<br>

## 3차원 합성곱
2차원 CNN 구조와 동일하나 깊이(채널)이 있는 경우로 Color 이미지(Red,Green,Blue)<br>
합성곱 계산은 (3,3,3) 영역에 해당하는 27개의 가중치를 곱하고 절편을 더함<br>
입력이나 필터의 차원이 몇 개인지 상관없이 항상 출력은 하나의 값(특성 맵에 한 원소)<br>
keras의 합성곱은 3차원이 기본<br>
패션 MNIST 같이 흑백인 경우 깊이(채널)이 1이 되어서 (28,28,1)<br>

## 여러 개의 필터가 있는 3차원 합성곱
CNN은 너비와 높이는 줄어들고 깊이는 깊어지는 특징을 갖고 있음<br>
CN 필터는 이미지에 있는 어떤 특징을 찾는다고 생각 가능<br>


## 합성곱 층
- ex) Conv2D => MaxPooling => Conv2D => MaxPooling => flatten(1차원으로 변경) => Dense => softmax => classification
- 첫번째 합성곱 => 두번째 합성곱 => FC층
- Dropout(x) => Overfitting 막기 위함(과적합을 막기위해 랜덤으로 가중치를 버린다.)

## 가중치 시각화
합성곱 층은 여러 개 필터를 사용해 이미지에서 특징을 학습<br>
필터는 커널이라 부르는 가중치와 절편을 갖고 있음<br>
가중치는 입력 이미지의 2차원 영역에 적용, 어떤 특징을 크게 두드러지게 표현하는 역할을 함<br>
