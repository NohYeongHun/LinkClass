# 인공신경망

1. 데이터셋 구하기.
```python
from tensorflow import keras

# mnist 데이터셋.
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() # 훈련세트, 테스트세트 분리
```
- keras Dataset 내장 데이터셋(https://keras.io/api/datasets)

2. 데이터의 분류
- train data 와 test data 분리
- train data를 train data, validation data 로 분리
- 교차검증 훈련 => test 데이터로 테스트.


3. 훈련 데이터셋 이미지 출력
```python
# 10 개 훈련 데이터셋 이미지 출력
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')
    axs[i].axis('off')
plt.show()

# 10개 훈련 데이터셋 타겟값 출력
print([train_target[i] for i in range(10)])

# 타겟값별 데이터 수 출력
import numpy as np

print(np.unique(train_target, return_counts=True))
```

4. 로지스틱 회귀로 패션 아이템 분류하기
```python
# 훈련 데이터 변환(각 이미지를 1차원으로)
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)
print(train_scaled.shape)

# LogisticRegression() 대신 확률적경사하강법(SGD)이용한 로지스틱회귀 분류 모델
# loss='log'는 이진분류는 sigmoid, 다중분류는 OvR(OvA) 이진분류 반복 후 softmax(10개의 z값)
from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier # optimizer

sc = SGDClassifier(loss='log', max_iter=5, random_state=42)

# 생성된 모델(sc)에 교차검증(기본 5-fold, 검증 데이터 82% 정확도)
scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))
```

5. 로지스틱회귀와 인공신경망(ANN)비교
- 인공신경망은 확률적경사하강법을 사용하는 로지스틱회귀와 같음.
- 확률적 경사하강법(Sigmoid, Softmax)
- 입력층(inupt layer)
- 출력층(output layer)

6. 인공신경망
```python
# 텐서플로와 케라스
import tensorflow as tf
from tensorflow import keras

# 인공신경망으로 모델 만들기
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled.shape, train_target.shape)
print(val_scaled.shape, val_target.shape)
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
model = keras.Sequential(dense)

# 인공신경망으로 패션 아이템 분류하기.
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

# 분류 예시
print(train_target[:10])

# 훈련 epoch=5
model.fit(train_scaled, train_target, epochs=5)

# 평가
model.evaluate(val_scaled, val_target)
```
## 엔트로피
- 엔트로피 : 불확실성의 척도
- 엔트로피가 높다는 것은 정보가 많고, 확률이 낮다는 것을 의ㅣㅁ

## 크로스 엔트로피
- 실제 분포 q에 대하여 알지 못하는 상태에서, 모델링을 통하여 구한 분포인 p를 통하여 q를 예측하는 것
- p(x)와 q(x)가 서로 교차해서 곱한다는 의미에서 크로스 엔트로피라고 함
- 머신러닝의 모델은 몇%의 확률로 예측.
- fit() - 모델 훈련(epoch 5, loss는 줄어들면서 정확도는 높아짐.)
- keras는 기본적으로 Mini-batch 경사하강법 사용(기본 batch_size는 32)



## 인공신경망의 작동방식(퍼셉트론)
1. 입력값(input layer)
2. 가중치(w)
3. 활성함수(f(x))
4. 출력값(output layer)
- 들어온 입력값과 각 가중치를 곱하고
- 곱해진 값들을 모두 더한 후
- 그 값이 한계값을 넘어서면 1 아니면 0을 출력.
- 원하는 입력값 결정.
- 가중치가 업데이트 되어가는 과정이 인공지능의 학습.

5. 해결 불가능한 과제 (XOR연산)
- 두 입력값이 같으면0, 다르면 1을 출력
- 해결방안 : 다중 퍼셉트론
- 오차가 생기면 거슬러 올라가 결정하는 오차역전파법.
- XOR 문제를 해결하려면 새로운 접근이 필요함.

6. 딥러닝
- 스스로 특징을 추출하고 분류한다.
- y = wx +b(w는 가중치, b는 바이어스)


## 다층 퍼셉트론(MLP)
- 은닉층을 포함해 가중치와 바이어스를 2차원 배열 나타낼 수 있음.
- XOR문제 해결
- XOR를 해결하는 가중치와 바이어스의 조합은 무수히 많음.

## 오차 역전파
- 원하는 XOR 문제의 정답이 도출됨.
- 신경망 내부의 가중치는 오차 역전파 방법을 사용해 수정.
- XOR에서 정답에 해당하는 w,b를 미리 알아본 후 이를 집어넣음
- 실제 프로젝트에서는 최적의 가중치와 바이어스를 찾아나가는 것.
이때 경사하강법 사용

## 드롭아웃
- 신경망에서 훈련시 Overfitting을 줄이기 위해 특화된 방법.
- 일정부분의 신경망만 적용시켜서 Overfitting을 줄인다.

## 모델 저장과 복원
1. 모델 저장
- 모델 파라미터(가중치, 절편) 저장 save_weights()
- 파일 확장자를 .h5이면 HDF5 포맷으로 저장
model.save_weights('model-weights.h5)

- 모델 구조와 파라미터 함께 저장 save()
- 파일 확장자를 .h5이면 HDF5 포맷으로 저장
model.save('model-whole.h5)

2. 모델 복원.
model.load('model-weights.h5')

## 조기 종료(early stopping)
- 최상의 점수를 얻으면 epoch 설정에 상관없이 종료
- 콜백을 2개 선언 (ModelCheckpoint(), EarlyStopping())

- 콜백을 포함해서 훈련
```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5')
early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,
                                                  restore_best_weights=True)
history = model.fit(train_scaled, train_target, epochs=20, verbose=0, 
                    validation_data=(val_scaled, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```


