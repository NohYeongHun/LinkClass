- 선형회귀에서 성능과 손실(loss)함수
- MSE(평균 제곱 오차)
- RMSE => MSE에 루트를 씌운 값

# Decision Tree
- 장점
- 설명이 가능하다.
- 원본 데이터로 구성이 가능하다.

## 지니 불순도(gini impurity)
- 결정트리의 분할 기준은 불순도(음성, 양성이 섞여있는 정도)
- 각 노드의 복잡성 불순도가 낮아지는 방향(순도가 높아지는)
- Leaf 노드가 순수해 질 때까지 계속 분할
- 결정트리는 분할 기준을 학습하는 것임(선형회귀, 로지스틱과 다름)
- 노드에서 데이터 분할 기준이 criterion 값(기본 값 gini)
```python
dt = DecisionTreeClassfier(random state=4, criterion='gini')
```
- 루트 노드의 지니 불순도
- 음성 양성이 반, 가장 불순함.
- 한쪽이 전체, 가장 순수함, 순수노드
- 부모, 자식노드 불순도차이, 정보이득(information gain)
- 불순도 차이가 크도록 트리를 성장시킴.

## 엔트로피 불순도(impurity)
- 노드에서 데이터 분할 기준인 criterion는 기본 gini외에 entropy가 있음.
- 일반적으로 기본값 gini와 entropy가 만든 결과의 차이는 크지 않음.
```python
dt = DecisionTreeClassifier(random_state=4, criterion='entropy')
dt.fit(train_scaled, train_target)
```
- Leaf가 순수해 질 때까지 계속 분할
- Overfitting 발생 가능성, 가지치기 필요

## 가지치기(Pruning)
- 과수원에서, 열매를 잘 맺게 하기 위해 가지치기 하는 것과 같음
- 무작정 끝까지 가지가 자라나는 트리 - 훈련세트에만 잘 맞는 트리가 생성
- 가지치기 중 가장 간단한 방법, 트리의 최대 깊이를 지정하는 것
- score는 낮아졌지만, 훈련, 테스트차이는 줄어 듬
- depth=1 에서 당도 기준
- depth=2, 왼쪽만 당도, 알코올, ph를 사용
- depth=3, 3번째만 음성(붉은색) Red Wine